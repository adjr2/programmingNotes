{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## [Getting Started With Testing in Python](https://realpython.com/python-testing/)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- **Exploratory testing** is a form of testing that is done without a plan. In an exploratory test, you’re just exploring the application.\n",
    "- **Automated testing** is the execution of your test plan (the parts of your application you want to test, the order in which you want to test them, and the expected responses) by a script instead of a human.\n",
    "- Testing multiple components in your application operate with each other is known as **integration testing**. Challenge: hard to diagnose the issue without being able to isolate which part of the system is failing.\n",
    "- A **unit test** is a smaller test, one that checks that a single component operates in the right way. A unit test helps you to isolate what is broken in your application and fix it faster."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "ename": "AssertionError",
     "evalue": "should be 6",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAssertionError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-5-3e066fc810f1>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;31m# unit test with `assert`\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[1;32massert\u001b[0m \u001b[0msum\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;36m2\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;36m3\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;36m6\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m\"should be 6\"\u001b[0m \u001b[1;31m# this won't print anything as assertion is correct\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 3\u001b[1;33m \u001b[1;32massert\u001b[0m \u001b[0msum\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;36m6\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m\"should be 6\"\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      4\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[1;31m# better way of writing this. save the following in a file named (say) test_sum.py\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mAssertionError\u001b[0m: should be 6"
     ]
    }
   ],
   "source": [
    "# unit test with `assert`\n",
    "assert sum([1,2,3]) == 6, \"should be 6\" # this won't print anything as assertion is correct\n",
    "assert sum([1,1,1]) == 6, \"should be 6\"\n",
    "\n",
    "# better way of writing this. save the following in a file named (say) test_sum.py\n",
    "def test_sum():\n",
    "    assert sum([1,2,3]) == 6, \"should be 6\"\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    test_sum()\n",
    "    print(\"everything is fine\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "ename": "AssertionError",
     "evalue": "Should be 6",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAssertionError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-13-1ad741931958>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      8\u001b[0m \u001b[1;32mif\u001b[0m \u001b[0m__name__\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;34m\"__main__\"\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      9\u001b[0m     \u001b[0mtest_sum\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 10\u001b[1;33m     \u001b[0mtest_sum_tuple\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     11\u001b[0m     \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"Everything passed\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-13-1ad741931958>\u001b[0m in \u001b[0;36mtest_sum_tuple\u001b[1;34m()\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[1;32mdef\u001b[0m \u001b[0mtest_sum_tuple\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 6\u001b[1;33m     \u001b[1;32massert\u001b[0m \u001b[0msum\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m2\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m2\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;36m6\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m\"Should be 6\"\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      7\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      8\u001b[0m \u001b[1;32mif\u001b[0m \u001b[0m__name__\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;34m\"__main__\"\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mAssertionError\u001b[0m: Should be 6"
     ]
    }
   ],
   "source": [
    "# extension of the previous example\n",
    "def test_sum():\n",
    "    assert sum([1, 2, 3]) == 6, \"Should be 6\"\n",
    "\n",
    "def test_sum_tuple():\n",
    "    assert sum((1, 2, 2)) == 6, \"Should be 6\"\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    test_sum()\n",
    "    test_sum_tuple()\n",
    "    print(\"Everything passed\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Writing tests in this way is okay for a simple check, but what if more than one fails? This is where test runners come in. The **test runner** is a special application designed for running tests, checking the output, and giving you tools for debugging and diagnosing tests and applications."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### unittest\n",
    "- `unittest` contains both a testing framework and a test runner. It requires that:\n",
    "    - You put your tests into classes as methods\n",
    "    - You use a series of special assertion methods in the `unittest.TestCase` class instead of the built-in assert statement."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      ".F\n",
      "======================================================================\n",
      "FAIL: test_sum_tuple (__main__.TestSum)\n",
      "----------------------------------------------------------------------\n",
      "Traceback (most recent call last):\n",
      "  File \"<ipython-input-15-ff62fe338b67>\", line 11, in test_sum_tuple\n",
      "    self.assertEqual(sum((1, 2, 2)), 6, \"Should be 6\")\n",
      "AssertionError: 5 != 6 : Should be 6\n",
      "\n",
      "----------------------------------------------------------------------\n",
      "Ran 2 tests in 0.003s\n",
      "\n",
      "FAILED (failures=1)\n"
     ]
    }
   ],
   "source": [
    "# converting the previous example to a `unittest` test case.\n",
    "import unittest\n",
    "\n",
    "\n",
    "class TestSum(unittest.TestCase):\n",
    "\n",
    "    def test_sum(self):\n",
    "        self.assertEqual(sum([1, 2, 3]), 6, \"Should be 6\")\n",
    "\n",
    "    def test_sum_tuple(self):\n",
    "        self.assertEqual(sum((1, 2, 2)), 6, \"Should be 6\")\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    # unittest.main() # this didn't work. more: https://stackoverflow.com/questions/37895781/unable-to-run-unittests-main-function-in-ipython-jupyter-notebook\n",
    "    unittest.main(argv=['first-arg-is-ignored'], exit=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Interpretation: one success (indicated with .) and one failure (indicated with F)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### nose\n",
    "- skipped\n",
    "##### pytest\n",
    "- supports execution of `unittest` test cases. `pytest` test cases are a series of functions in a Python file starting with the name `test_`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# writing test in pytest is similar to test functions defined above\n",
    "def test_sum():\n",
    "    assert sum([1, 2, 3]) == 6, \"Should be 6\"\n",
    "\n",
    "def test_sum_tuple():\n",
    "    assert sum((1, 2, 2)) == 6, \"Should be 6\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Writing Your First Test"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Create a new folder called `my_sum`. Inside my_sum, create an empty file called `__init__.py`. Creating the `__init__.py` file means that the `my_sum` folder can be imported as a module from the parent directory.\n",
    "- Create a new function called `sum()` in `my_sum/__init__.py`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Where to Write the Test\n",
    "- Because the test file will need to be able to import your application to be able to test it, you want to place `test_with_pytest.py` above the package folder (inside directly inside `testing/`).\n",
    "\n",
    "Note: You can import any attributes of the script, such as classes, functions, and variables by using the built-in `__import__()` function. Instead of from `my_sum` import sum, you can write the following:\n",
    "\n",
    "```python\n",
    "target = __import__(\"my_sum.py\")\n",
    "sum = target.sum\n",
    "```\n",
    "The benefit of using `__import__()` is that you don’t have to turn your project folder into a package, and you can specify the file name. This is also useful if your filename collides with any standard library packages. For example, `math.py` would collide with the `math` module."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### How to Structure a Simple Test\n",
    "- Couple of decisions:\n",
    "    1. What do you want to test?\n",
    "    2. Are you writing a unit test or an integration test?\n",
    "- Workflow:\n",
    "    1. Create your inputs.\n",
    "    2. Execute the code being tested, capturing the output.\n",
    "    3. Compare the output with an expected result."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### How to Write Assertions\n",
    "- **Assertion**: writing a test is to validate the output against a known response. There are some general best practices around how to write assertions:\n",
    "    - Make sure tests are repeatable and run your test multiple times to make sure it gives the same result every time\n",
    "    - Try and assert results that relate to your input data, such as checking that the result is the actual sum of values in the sum() example\n",
    "- `unittest` comes with lots of methods to assert on the values, types, and existence of variables. Examples: `.assertEqual(a, b)`, `.assertTrue(x)`, `.assertIs(a, b)` etc."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Side Effects\n",
    "- Executing a piece of code will alter other things in the environment, such as the attribute of a class, a file on the filesystem, or a value in a database. These are known as **side effects** and are an important part of testing. Decide if the side effect is being tested before including it in your list of assertions.\n",
    "- If you find that the unit of code you want to test has lots of side effects, you might be breaking the **Single Responsibility Principle**. Breaking the Single Responsibility Principle means the piece of code is doing too many things and would be better off being refactored."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Executing Your First Test\n",
    "#### Executing Test Runners\n",
    "- The following will call `unittest.main()`:\n",
    "```bash\n",
    "python test_with_pytest.py\n",
    "```\n",
    "- Another way:\n",
    "```bash\n",
    "cd testing/ # or location of test_with_pytest.py file\n",
    "python -m unittest test #test is the file name (test_with_pytest.py in this example)\n",
    "```\n",
    "\n",
    "- The following will search the current directory for any files named `test*.py` and attempt to test them.\n",
    "```bash\n",
    "python -m unittest discover\n",
    "```\n",
    "\n",
    "- You can provide the name of the directory by using the `-s` flag and the name of the directory:\n",
    "```bash\n",
    "python -m unittest discover -s tests # there is no tests directory in this example\n",
    "```\n",
    "\n",
    "-  If your source code is not in the directory root and contained in a subdirectory, for example in a folder called `src/`, you can tell unittest where to execute the tests so that it can import the modules correctly with the `-t` flag:\n",
    "```bash\n",
    "python -m unittest discover -s tests -t src # there is no tests or src directory in this example\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### How to Use the Django Test Runner\n",
    "- The Django startapp template will have created a tests.py file inside your application directory (otherwise create it). It has the following content:\n",
    "```python\n",
    "from django.test import TestCase\n",
    "\n",
    "class MyTestCase(TestCase):\n",
    "    # Your test methods\n",
    "```\n",
    "\n",
    "- The major difference: you need to inherit from the `django.test.TestCase` instead of `unittest.TestCase`. These classes have the same API, but the Django TestCase class sets up all the required state to test.\n",
    "- To execute your test suite use `manage.py` test:\n",
    "```bash\n",
    "python manage.py test\n",
    "```\n",
    "- If you want multiple test files, replace `tests.py` with a folder called `tests`, insert an empty file inside called `__init__.py`, and create your `test_*.py` files. Django will discover and execute these."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### More Advanced Testing Scenarios\n",
    "- The data that you create as an input is known as a **fixture**. It’s common practice to create fixtures and reuse them.\n",
    "- If you’re running the same test and passing different values each time and expecting the same result, this is known as **parameterization**.\n",
    "\n",
    "##### Handling Expected Failures\n",
    "- There’s a special way to handle expected errors. You can use ``.assertRaises()` as a context-manager."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Isolating Behaviors in Your Application\n",
    "- There are some simple techniques you can use to test parts of your application that have many side effects:\n",
    "    - Refactoring code to follow the Single Responsibility Principle\n",
    "    - Mocking out any method or function calls to remove side effects\n",
    "    - Using integration testing instead of unit testing for this piece of the application"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Writing Integration Tests\n",
    "- Integration testing is the testing of multiple components of the application to check that they work together. Integration testing might require acting like a consumer or user of the application by:\n",
    "    - Calling an HTTP REST API\n",
    "    - Calling a Python API\n",
    "    - Calling a web service\n",
    "    - Running a command line\n",
    "\n",
    "- Each of these types of integration tests can be written in the same way as a unit test, following the Input, Execute, and Assert pattern. The most significant difference is that integration tests are checking more components at once and therefore will have more side effects than a unit test. Also, integration tests will require more fixtures to be in place, like a database, a network socket, or a configuration file.\n",
    "- This is why it’s good practice to separate your unit tests and your integration tests. The creation of fixtures required for an integration like a test database and the test cases themselves often take a lot longer to execute than unit tests, so you may only want to run integration tests before you push to production instead of once on every commit."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Testing Data-Driven Applications\n",
    "- These types of integration tests will depend on different test fixtures to make sure they are repeatable and predictable.\n",
    "- A good technique to use is to store the test data (like a `json`) in a folder within your integration testing folder called `fixtures` to indicate that it contains test data. Then, within your tests, you can load the data and run the test.\n",
    "- Within your test case, you can use the ``.setUp()` method to load the test data from a fixture file in a known path and execute many tests against that test data.\n",
    "- If your application depends on data from a remote location, like a `remote API`, you’ll want to ensure your tests are repeatable. Having your tests fail because the API is offline or there is a connectivity issue could slow down development. In these types of situations, it is best practice to store remote fixtures locally so they can be recalled and sent to the application. The `requests` library has a complimentary package called `responses` that gives you ways to create response fixtures and save them in your test folders. Find out more on their [GitHub Page](https://github.com/getsentry/responses)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Testing in Multiple Environments\n",
    "- `Tox` is an application that automates testing in multiple environments.\n",
    "- Read when required."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Automating the Execution of Your Tests\n",
    "- There are some tools for executing tests automatically when you make changes and commit them to a source-control repository like Git. Automated testing tools are often known as CI/CD tools, which stands for “Continuous Integration/Continuous Deployment.” They can run your tests, compile and publish any applications, and even deploy them into production.\n",
    "- Explained Travis CI (similar to gitlab tool), read when required."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Introducing Linters Into Your Application\n",
    "- A **linter** will look at your code and comment on it. It could give you tips about mistakes you’ve made, correct trailing spaces, and even predict bugs you may have introduced.\n",
    "\n",
    "##### Passive Linting With flake8\n",
    "- A popular linter that comments on the style of your code is `flake8`.\n",
    "- `flake8` is configurable on the command line or inside a configuration file in your project If you wanted to ignore certain rules you can set them in the configuration. `flake8` will inspect a `.flake8` file in the project folder or a `setup.cfg` file.\n",
    "```\n",
    "[flake8]\n",
    "ignore = E305 # ignores E305 (expected 2 blank lines after class or function definition)\n",
    "exclude = .git,__pycache__ # ignores the .git and __pycache__ directories\n",
    "max-line-length = 90\n",
    "```\n",
    "- Alternatively, you can provide these options on the command line:\n",
    "```bash\n",
    "flake8 --ignore E305 --exclude .git,__pycache__ --max-line-length=90\n",
    "```\n",
    "\n",
    "##### Aggressive Linting With a Code Formatter\n",
    "- `flake8` is a passive linter: it recommends changes, but you have to go and change the code. A more aggressive approach is a **code formatter**. Code formatters will change your code automatically to meet a collection of style and layout practices.\n",
    "- `black` is a very unforgiving formatter. It doesn’t have any configuration options, and it has a very specific style. This makes it great as a drop-in tool to put in your test pipeline."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Keeping Your Test Code Clean\n",
    "- Over time, you will develop a lot of **technical debt** in your test code, and if you have significant changes to your application that require changes to your tests, it can be a more cumbersome task than necessary because of the way you structured them. Try to follow the **DRY** principle when writing tests: **Don’t Repeat Yourself**.\n",
    "- Test fixtures and functions are a great way to produce test code that is easier to maintain. Also, readability counts."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Testing for Performance Degradation Between Changes\n",
    "- There are many ways to benchmark code in Python. The standard library provides the timeit module, which can time functions a number of times and give you the distribution. This example will execute test() 100 times and print() the output:\n",
    "```python\n",
    "def test():\n",
    "    # ... your code\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    import timeit\n",
    "    print(timeit.timeit(\"test()\", setup=\"from __main__ import test\", number=100))\n",
    "```\n",
    "\n",
    "- Another option, if you decided to use `pytest` as a test runner, is the `pytest-benchmark` plugin. This provides a `pytest` fixture called `benchmark`. You can pass `benchmark()` any callable, and it will log the timing of the callable to the results of `pytest`. Then, you can add a test that uses the fixture and passes the callable to be executed:\n",
    "```python\n",
    "def test_my_function(benchmark):\n",
    "    result = benchmark(test)\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Testing for Security Flaws in Your Application\n",
    "- Another test you will want to run on your application is checking for common security mistakes or vulnerabilities. You can install `bandit`.\n",
    "- You can then pass the name of your application module with the -r flag, and it will give you a summary:\n",
    "```bash\n",
    "bandit -r my_sum\n",
    "```\n",
    "- The rules that bandit flags are configurable, and if there are any you wish to ignore, you can add the following section to your setup.cfg file with the options:\n",
    "```config\n",
    "[bandit]\n",
    "exclude: /test\n",
    "tests: B101,B102,B301\n",
    "```\n",
    "- [More](https://github.com/PyCQA/bandit)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.2"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
